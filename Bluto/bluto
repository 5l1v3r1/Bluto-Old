#!/usr/local/bin/python

from multiprocessing.dummy import Pool as ThreadPool 
from bs4 import BeautifulSoup
from termcolor import colored
import dns.resolver
import dns.query
import dns.zone
import sys
import time
import datetime
import socket
import os
import math
import requests
import re
import collections
import threading
import urllib2
import hashlib
import json
import site
import random
import string
import Queue

targets = []
prox = True
default_s = False
myResolver = dns.resolver.Resolver()
myResolver.nameservers = ['8.8.8.8']
version = "1.1.13"

sites = site.getsitepackages()
for item in sites:
    if os.path.exists(item + "/Bluto/doc/subdomains-top1mil-20000.txt"):
        path = item
    else:
        pass

filename1 = path + "/Bluto/doc/subdomains-top1mil-20000.txt"
#filename1 = path + "/Bluto/doc/subdomains-top1mil.txt"
filename2 = path + "/Bluto/doc/sub_interest.txt"
useragent_f = path + "/Bluto/doc/user_agents.txt"
countries_file = path + "/Bluto/doc/countries.txt"

def action_output_wild_false(brute_results_dict, sub_intrest, google_results, bing_true_results,linkedin_results, check_count):
    
    linkedin_evidence_results = []
    email_evidence_results = []
    email_results = []
    url_seen = []
    person_seen = []
    final_emails = []
    
    for e, u in google_results:
        if e not in email_results:
            email_results.append(e)
        if e not in url_seen:
            email_evidence_results.append((e, u))
    for e, u in bing_true_results:
        email_results.append(e)
        if u not in url_seen:
            email_evidence_results.append((e, u))        
              
    for email, url_href, person in linkedin_results:
        if person not in person_seen:
            person_seen.append(person)
        if email not in email_results:
                email_results.append(str(email).lower())
        if url_href not in url_seen:
            url_seen.append(url_href)
            linkedin_evidence_results.append((email, url_href, person))
            email_evidence_results.append((email, url_href))
            
    linkedin_evidence_results.sort(key=lambda tup: tup[2])   
    sorted_email = sorted(email_results)
    for email in sorted_email:
        if email == '[]':
            pass
        else:
            final_emails.append(email)
    email_count = len(final_emails)
    staff_count = len(person_seen)
    
    print '\nEmail Addresses:\n'
    for email in final_emails:
        if email == '[]':
            pass
        else:
            print email    
    
    print '\nLinkedIn Results:\n'
    
    for email, url_href, person in linkedin_results:
        if person not in person_seen:
            person_seen.append(person)   
    
    sorted_person = sorted(person_seen)
    for person in sorted_person:
        print person
    
        
    sorted_dict = collections.OrderedDict(sorted(brute_results_dict.items()))
    print "\nBluto Results: \n"
    for item in sorted_dict:
        if item in sub_intrest:
            print colored(item + "\t", 'red'), colored(sorted_dict[item], 'red')
        else:
            print item + "\t",sorted_dict[item]
    
    print '\nStats and Evidence Report:'
    print '\n\tEmail Evidence:\n'
    for email, url in email_evidence_results:
        if email == []:
            pass
        else:
            print email
            print url
    
    print '\n\tLinkedIn Evidence:\n'
    for email, url_href, person in linkedin_evidence_results:
        print person
        print url_href
        if email == []:
            pass
        else:
            print email
    time_spent_email_f = str(datetime.timedelta(seconds=(time_spent_email))).split('.')[0]
    time_spent_brute_f = str(datetime.timedelta(seconds=(time_spent_brute))).split('.')[0]
    time_spent_total_f = str(datetime.timedelta(seconds=(time_spent_total))).split('.')[0]
    print '\nPotential Emails Found: {}' .format(str(email_count))
    print 'Potential Staff Members Found: {}' .format(str(staff_count))
    print "Email Enumeration:", time_spent_email_f
    print "Requests executed:", str(check_count) + " in ", time_spent_brute_f
    print "Total Time:", time_spent_total_f, '\n'    
    
                
def action_zone_transfer(zn_list, domain):
    print "\nAttempting Zone Transfers"
    zn_list.sort()
    vuln = True
    vulnerable_listT = []
    vulnerable_listF = []
    dump_list = []
    for ns in zn_list:
        try:
            z = dns.zone.from_xfr(dns.query.xfr(ns, domain))
            names = z.nodes.keys()
            names.sort()
            if vuln == True:
                vulnerable_listT.append(ns)
                            
        except Exception as e:
            error = str(e)
            if error == "[Errno 54] Connection reset by peer" or "No answer or RRset not for qname":
                vuln = False
                vulnerable_listF.append(ns)
            else:
                print """An unexpected error has occured. Please report the error and it's context to https://github.com/RandomStorm/Bluto/issues, thank you."""
                print error          
                
    
    if vulnerable_listF:
        print "\nNot Vulnerable:\n"
        for ns in vulnerable_listF:
            print colored(ns, 'green')
    
    if vulnerable_listT:
        print "\nVulnerable:\n"
        for ns in vulnerable_listT:
            print colored(ns,'red'), colored("\t" + "TCP/53", 'red')

   
        z = dns.zone.from_xfr(dns.query.xfr(vulnerable_listT[0], domain))
        names = z.nodes.keys()
        names.sort()
        print "\nRaw Zone Dump\n"
        for n in names:
            data1 = "{}.{}" .format(n,domain)
            try:
                addr = socket.gethostbyname(data1)
                dump_list.append("{}.{} {}" .format(n, domain, addr))
            
            except Exception as e:
                error = str(e)
                if error == "[Errno -5] No address associated with hostname":
                    pass
                else:
                    print """An unexpected error has occured. Please report the error and it's context to https://github.com/RandomStorm/Bluto/issues, thank you."""
                    print error + ': in "action_zone_transfer"\n'
            print z[n].to_text(n)

        clean_dump = sorted(set(dump_list))
        target_dict = dict((x.split(' ') for x in clean_dump))
        clean_target = collections.OrderedDict(sorted(target_dict.items()))
        print "\nProcessed Dump\n"
        for item in clean_target:
            if item in sub_intrest:
                print colored(item, 'red'), colored("\t" + clean_target[item], 'red')
            else:
                print item, "\t" + target_dict[item]
    
    
    return vulnerable_listT

def get_dns_details(domain):
    ns_list = []
    zn_list =[]
    mx_list = []        
    try:
        print "\nName Server:\n"
        myAnswers = myResolver.query(domain, "NS")
        for data in myAnswers.rrset:
            data1 = str(data)
            data2 = (data1.rstrip('.'))
            addr = socket.gethostbyname(data2)
            ns_list.append(data2 + '\t' + addr)
            zn_list.append(data2)
            list(set(ns_list))
            ns_list.sort()
        for i in ns_list:
            print colored(i, 'green')
    except:
        e = str(sys.exc_info()[0])
        print "\nCheck The Target Domain == Correct!\n\nQuitting.."
        sys.exit()
        
    try:    
        print "\nMail Server:\n"
        myAnswers = myResolver.query(domain, "MX")
        for data in myAnswers:
            data1 = str(data)
            data2 = (data1.split(' ',1)[1].rstrip('.'))
            addr = socket.gethostbyname(data2)
            mx_list.append(data2 + '\t' + addr)
            list(set(mx_list))
            mx_list.sort()
        for i in mx_list:
            print colored(i, 'green')
    except:
        e = str(sys.exc_info()[0])
        print "\tNo Mail Servers"   
             
    return zn_list

def get_line_count(filename):
    lines = 0
    for line in open(filename):
        lines += 1
    return lines

def action_wild_cards(domain):
    try:
        one = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(15))
        myAnswers = myResolver.query(str(one) + '.' + str(domain))
    
    except dns.resolver.NoNameservers:
        print '\n\tNo Name Servers'
        pass
        
    except dns.resolver.NXDOMAIN:
        return False
    else:
        return True    

def action_brute(subdomain):
    try:   
        myAnswers = myResolver.query(subdomain)
        for data in myAnswers:
            targets.append(subdomain + ' ' + str(data))
            
    except Exception as e:
        pass
    
    
def get_subs(filename):
    full_list = []
    try:    
        subs = [line.rstrip('\n') for line in open(filename)]
        for sub in subs:
            full_list.append(str(sub.lower() + "." + domain))
    except Exception as e:
        error = str(e)
        print """An unexpected error has occured. Please report the error and it's context to https://github.com/RandomStorm/Bluto/issues, thank you."""
        print error + ': in "get_subs"\n'
        sys.exit()
        
    return full_list    
        
def get_sub_interest(filename):
    full_list = []
    try:
        subs = [line.rstrip('\n') for line in open(filename)]
        for sub in subs:
            full_list.append(str(sub.lower() + "." + domain))
    
    except Exception as e:
        error = str(e)
        print """An unexpected error has occured. Please report the error and it's context to https://github.com/RandomStorm/Bluto/issues, thank you."""
        print error + ': in "get_sub_interest"\n'
        sys.exit()
        
    return full_list


def action_netcraft(domain):
    netcraft_list = []
    print "\nPassive Gatherings From NetCraft\n"
    try:
        res = NetcraftAPI({'verbose': True}).search(domain)
    except Exception as e:
        print e
        sys.exit()
    
    for item in res:
        data1 = str(item + "." + domain)
        try:
            addr = socket.gethostbyname(data1)
            netcraft_list.append(item + "." + domain + " " + addr)
        except Exception as e:
            error = str(e)
            print """An unexpected error has occured. Please report the error and it's context to https://github.com/RandomStorm/Bluto/issues, thank you."""
            print error
            continue
            
    netcraft_list.sort()
        
    for item in netcraft_list:
            print colored(item, 'red')
            
    return netcraft_list

def get_user_agents(useragent_f):
    uas = []
    with open(useragent_f, 'rb') as uaf:
        for ua in uaf.readlines():
            if ua:
                uas.append(ua.strip()[1:-1-1])
    random.shuffle(uas)
    return uas

def action_country_id():
    userCountry = ''
    userServer = ''
    userIP = ''
    userID = False
    o = 0    
    tcountries_dic = {}
    country_list = []
    
    with open(countries_file) as fin:
        for line in fin:
            key, value = line.strip().split(';')
            tcountries_dic.update({key: value})
    
    countries_dic = dict((k.lower(), v.lower()) for k,v in tcountries_dic.iteritems())

    for country, server in countries_dic.items():
        country_list.append(country)
    
    country_list = [item.capitalize() for item in country_list]
    country_list.sort()
    
    while True:
        try:
            if prox == True:
                proxy = {'http' : 'http://127.0.0.1:8080'}
                r = requests.get(r'http://ip.pycox.com/json', proxies=proxy)
                ip = r.json()['q']
                originCountry = r.json()['country_name'] 
                
            else:  
                r = requests.get(r'http://ip.pycox.com/json')
                ip = r.json()['q']
                originCountry = r.json()['country_name']
                
        except ValueError as e:
            if o == 0:
                print colored('\nUnable to connect to the CountryID, we will retry.', 'red')
            if o > 0:
                print '\nThis is {} of 3 attempts' .format(o)
            time.sleep(2)
            o += 1
            if o == 4:
                break
            continue
        break
 
    if o == 4:
        print colored('\nWe have been unable to connect to the CountryID service.\n','red')
        print '\nPlease let Bluto know what country you hale from.\n'
        print colored('Available Countries:\n', 'green')
        
        if len(country_list) % 2 != 0:
            country_list.append(" ")
        
        split = len(country_list)/2
        l1 = country_list[0:split]
        l2 = country_list[split:]
        
        for key, value in zip(l1,l2):
            print "{0:<20s} {1}".format(key, value)
        
        country_list = [item.lower() for item in country_list]
        
        while True:
            originCountry = raw_input('\nCountry: ').lower()
            if originCountry in country_list:
                break
            if originCountry == '':
                print '\nYou have not selected a country so the default server will be used'
                originCountry = 'United Kingdom'.lower()
                break
            else:
                print '\nCheck your spelling and try again'
                
        for country, server in countries_dic.items():
            if country == originCountry:
                userCountry = country
                userServer = server
                userID = True
            
    else:                                   
            
        for country, server in countries_dic.items():
            if country == originCountry.lower():
                userCountry = country
                userServer = server
                userID = True      
        if userID == False:
            if default_s == True:
                userCountry = 'DEAFULT'
                pass
            else:
                print 'Bluto currently doesn\'t have your countries google server available.\nPlease navigate to "http://www.telize.com/geoip/" and post an issue to "https://github.com/RandomStorm/Bluto/issues"\nincluding the country value as shown in the json output\nYou have been assigned to http://www.google.com for now.'
                userServer = 'http://www.google.co.uk'
                userCountry = 'United Kingdom'
    
    print '\n\tSearching From: {0}\n\tGoogle Server: {1}\n' .format(userCountry.title(), userServer)
    return (userCountry, userServer)

def action_linkedin(domain, q):
    uas = get_user_agents(useragent_f)
    email = []
    entries_tuples = []
    seen = set()
    results = []

    for start in range (0,50):
        ua = random.choice(uas)
        
        link = 'https://search.disconnect.me/searchTerms/search?'
        proxy = {'https' : 'https://127.0.0.1:8080'}
        
        headers = {'Connection' : 'close', 
         'User-Agent' : ua, 
         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 
         'Accept-Language': 'en-US,en;q=0.5',
         'Accept-Encoding': 'gzip, deflate',
         'Referer': 'http://www.google.com'}
        

        payload = {'start':'nav','option':'Web','query': 'site:linkedin.com/in ' + domain,'ses':'Google','location_option':'UK','nextDDG':'/search?q=&hl=en&start='+ str(start*10) + '&sa=N','showIcons':'false','filterIcons':'none','js_enabled':'1','source':'None'}
    
        try:
            if prox == True:
                requests.packages.urllib3.disable_warnings()
                response = requests.get(link, headers=headers, params=payload, proxies=proxy,verify=False)
            else:
                response = requests.get(link, headers=headers, params=payload)
                
            soup = BeautifulSoup(response.text, "html5lib")
            
            div = soup.find('div', {'id': 'results'})
            ul = div.find('ul', {'id': 'normal-results'})            
        except Exception as e:
            pass
    
        try:
            for li in ul.findAll('li'):
                title = li.find('a', {'class': 'title'})
                match2 = re.search('[a-zA-Z0-9.]*' + '@' + domain, title.text)
                url_href = title.get('href')
                nametemp = title.text
                person_temp = (nametemp.split(' | ')[0])
                person = re.match('\w+\s\w+',person_temp).group(0)
                email_text = li.findAll('p')               
                match = re.search('[a-zA-Z0-9.]*' + '@' + domain, str(email_text))
                if match:
                    if match:
                        email = match.group(0)
                entries_tuples.append((email, url_href, person.title()))
                        
        except Exception as e:
            pass   
        
    for person in entries_tuples:
        if person[2] not in seen:
            results.append(person)
            seen.add(person[2])
        
    q.put(sorted(results))

def action_google_disconnect(domain):
    entries_tuples = []
    seen = set()
    results = []
    ua = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'

    headers = {'Connection' : 'close', 
               'User-Agent' : ua, 
               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 
               'Accept-Language': 'en-US,en;q=0.5',
               'Accept-Encoding': 'gzip, deflate',
               'Referer': 'http://www.google.com'}
    
    link = 'https://search.disconnect.me/searchTerms/search?'
    proxy = {'https' : 'https://127.0.0.1:8080'}
    
    for start in range (0,50):
    
        payload = {'start':'nav','option':'Web','query': '@' + '"' + domain + '"','ses':'Google','location_option':'UK','nextDDG':'/search?q=&hl=en&start='+ str(start*10) + '&sa=N','showIcons':'false','filterIcons':'none','js_enabled':'1','source':'None'}
        
        if prox == True:
            requests.packages.urllib3.disable_warnings()
            response = requests.get(link, headers=headers, params=payload, proxies=proxy,verify=False)
        else:
            response = requests.get(link, headers=headers, params=payload)            
        
        soup = BeautifulSoup(response.text, "lxml")
        
        div = soup.find('div', {'id': 'results'})
        ul = div.find('ul', {'id': 'normal-results'})
        try:
            for li in ul.findAll('li'):
                title = li.find('a', {'class': 'title'})
                url_href = title.get('href')
                email_text = li.findAll('p')[1].text               
                match = re.search('[a-zA-Z0-9.]*' + '@' + '[a-zA-Z0-9.-]*' + domain, email_text)
                if match:
                    if match:
                        email = match.group(0)
                        entries_tuples.append((email,url_href))
                            
        except Exception as e:
            print 'Google Disconnect: ', e
            pass
        
    for urls in entries_tuples:
        if urls[1] not in seen:
            results.append(urls)
            seen.add(urls[1])
                
    q.put(sorted(results))


def action_google_true(domain, userCountry, userServer, q):
    uas = get_user_agents(useragent_f)
    searchfor = '@' + '"' + domain + '"'
    entries_tuples = []
    seen = set()
    results = []    
    for start in range(0,20):
        ua = random.choice(uas)
        try:
            if prox == True:
                proxy = {'http' : 'http://127.0.0.1:8080'}
            else:
                pass            
            headers = {"Connection" : "close", 
                       "User-Agent" : ua, 
                       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 
                       'Accept-Language': 'en-US,en;q=0.5',
                       'Accept-Encoding': 'gzip, deflate',
                       'Referer': 'http://www.google.com'}
            payload = { 'nord':'1', 'q': searchfor, 'start': start*10}

            link = '{0}/search?num=200' .format(userServer)
            if prox == True:
                response = requests.get(link, headers=headers, params=payload, proxies=proxy)
            else:
                response = requests.get(link, headers=headers, params=payload)
                
            soup = BeautifulSoup(response.text, "lxml")

            for cite in soup.select("li.g div.s div.kv cite"):
                span = cite.find_next("span", class_="st")
                match = re.search('[a-zA-Z0-9.]*' + '@' + domain, span.text)
                if match:
                    if match is not '@' or match is not '@' + domain:
                        email = match.group(0)
                        url_href = cite.text
                        entries_tuples.append((str(email).lower(),url_href))
 
            if str(response.status_code) == '503':
                print colored('Google is responding with a Captcha...\n', 'red')
                print colored('We will use an alterante method, however\nthe findings may not be as comprehensive\n','green')
                entries_tuples = search_google2(domain)
                for urls in entries_tuples:
                    if urls[1] not in seen:
                        results.append(urls)
                        seen.add(urls[1])
                q.put(sorted(results))
                break
            
            time.sleep(3)
            for urls in entries_tuples:
                if urls[1] not in seen:
                    results.append(urls)
                    seen.add(urls[1])         
                  
        except Exception as e:
            continue
        
    
    q.put(sorted(results))     

def action_bing_true(domain,q):
    emails = []
    uas = get_user_agents(useragent_f)
    searchfor = '@' + '"' + domain + '"'    
    for start in range(0,50):
        ua = random.choice(uas)
        if prox == True:
            proxy = {'http' : 'http://127.0.0.1:8080'}
        else:
            pass
        try:
            headers = {"Connection" : "close", 
                       "User-Agent" : ua,
                       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 
                       'Accept-Language': 'en-US,en;q=0.5',
                       'Accept-Encoding': 'gzip, deflate'}
            payload = { 'q': searchfor, 'first': start}
            link = 'http://www.bing.com/search'
            if prox == True:
                response = requests.get(link, headers=headers, params=payload, proxies=proxy)
            else:
                response = requests.get(link, headers=headers, params=payload)
            reg_emails = re.compile('[a-zA-Z0-9.-]*' + '@' + '<strong>')
            temp = reg_emails.findall(response.text)
            time.sleep(1)
            for item in temp:
                clean = item.replace("<strong>", "")
                email.append(clean + domain)  
                
        except Exception as e:
            continue  
    q.put(sorted(emails))
        
class NetcraftAPI(object):
    """
    This == the (unofficial) Python API for the netcraft.com Website.
    Using this code, you can retrieve subdomains. This has been modified from its 
    original state by the authors of Bluto, the original can be found on the following url. 
    https://github.com/PaulSec/API-netcraft.com
    
    """    

    """
        NetcraftAPI Main Handler
    """

    _instance = None
    _verbose = False

    def __init__(self, arg=None):
        pass

    def __new__(cls, *args, **kwargs):
        """
            __new__ builtin
        """
        if not cls._instance:
            cls._instance = super(NetcraftAPI, cls).__new__(
                cls, *args, **kwargs)
            if (args and args[0] and args[0]['verbose']):
                cls._verbose = True
        return cls._instance

    def display_message(self, s):
        if (self._verbose):
            print '%s' % s

    def search(self, domain):
        res = []
        url = "http://searchdns.netcraft.com/?restriction=site+contains&host=*.%s&lookup=wait..&position=limited" % domain
        s = requests.session()
        s.get('http://searchdns.netcraft.com/')
        req = s.get(url)

        challenge_cookie = req.headers['set-cookie'].split('=')[1].split(';')[0]
        string = urllib2.unquote(challenge_cookie)
        challenge_cookie_value = hashlib.sha1(string).hexdigest()
        cookies = {'netcraft_js_verification_response': challenge_cookie_value}

        req = s.get(url, cookies = cookies)
        soup = BeautifulSoup(req.content, "lxml")

        pattern = 'Found (\d+) site'
        number_results = re.findall(pattern, req.content)

        if (len(number_results) > 0 and number_results[0] != '0'):
            number_results = int(number_results[0])
            number_pages = int(math.ceil(number_results / 20)) + 1

            pattern = 'rel="nofollow">([a-z\.\-A-Z0-9]+)<FONT COLOR="#ff0000">'
            subdomains = re.findall(pattern, req.content)
            res.extend(subdomains)
            last_result = subdomains[-1]
            # "Last result: %s" % last_result

            for index_page in xrange(1, number_pages):
                url = "http://searchdns.netcraft.com/?host=*.%s&last=%s.%s&from=%s&restriction=site contains&position=limited" % (domain, last_result, domain, (index_page * 20 + 1))
                req = s.get(url, cookies = cookies)
                pattern = 'rel="nofollow">([a-z\-\.A-Z0-9]+)<FONT COLOR="#ff0000">'
                subdomains = re.findall(pattern, req.content)
                # print req.content
                res.extend(subdomains)
                try:
                    for subdomain in subdomains:
                        hostname = ('{}.{}' .format (subdomain, domain))
                        addr = socket.gethostbyname(hostname)                    
                        #self.display_message('{}.{} {}' .format (subdomain, domain, addr))
                except Exception as e:
                    pass
                last_result = subdomains[-1]
            return res
        else:
            self.display_message("\tNo results found for %s" % domain)
            return res



print """
BBBBBBBBBBBBBBBBB  lllllll                       tttt                          
B::::::::::::::::B l:::::l                     ttt:::t                          
B::::::BBBBBB:::::Bl:::::l                     t:::::t                          
BB:::::B     B:::::l:::::l{6}              t:::::t                          
  B::::B     B:::::Bl::::luuuuuu    uuuuuttttttt:::::ttttttt      ooooooooooo   
  B::::B     B:::::Bl::::lu::::u    u::::t:::::::::::::::::t    oo:::::::::::oo 
  B::::BBBBBB:::::B l::::lu::::u    u::::t:::::::::::::::::t   o:::::::::::::::o
  B:::::::::::::BB  l::::lu::::u    u::::tttttt:::::::tttttt   o:::::ooooo:::::o
  B::::BBBBBB:::::B l::::lu::::u    u::::u     t:::::t         o::::o     o::::o
  B::::B     B:::::Bl::::lu::::u    u::::u     t:::::t         o::::o     o::::o
  B::::B     B:::::Bl::::lu::::u    u::::u     t:::::t         o::::o     o::::o
  B::::B     B:::::Bl::::lu:::::uuuu:::::u     t:::::t    ttttto::::o     o::::o
BB:::::BBBBBB::::::l::::::u:::::::::::::::uu   t::::::tttt:::::o:::::ooooo:::::o
B:::::::::::::::::Bl::::::lu:::::::::::::::u   tt::::::::::::::o:::::::::::::::o
B::::::::::::::::B l::::::l uu::::::::uu:::u     tt:::::::::::ttoo:::::::::::oo 
BBBBBBBBBBBBBBBBB  llllllll   uuuuuuuu  uuuu       ttttttttttt    ooooooooooo
                                                                            
     {2} | {3} | {4} | {7}
            {0}  |  {1}
                 {5}
""" . format (colored("Author: Darryl Lane", 'blue'),colored("Twitter: @darryllane101", 'blue'),colored("DNS Recon", 'green'),colored("Brute Forcer", 'green'),colored("DNS Zone Transfers", 'green'),colored("https://github.com/RandomStorm/Bluto", 'green'),colored("v" + version, 'red'),(colored("Email Enumeration", 'green')))

if __name__ == "__main__":
    domain = raw_input("\nTarget Domain: ")
    emails = []
    
#Detail Call
    sub_intrest = get_sub_interest(filename2)
    zn_list = get_dns_details(domain)
#NetCraft Call
    netcraft_list = action_netcraft(domain)
#ZoneTrans Call    
    vulnerable_list = action_zone_transfer(zn_list, domain)
    if vulnerable_list == []:
        print "\nNone of the Name Servers are vulnerable to Zone Transfers"
#Bruting
        print '\nTesting For Wild Cards'
        value = action_wild_cards(domain)
        if value == True:
            email_address = []
            print colored('\n\tWild Cards Are In Place','green')
            print '\nGathering Email Addresses'
            userCountry, userServer = action_country_id()
            start_time_email = time.time()
            q1 = Queue.Queue()
            q2 = Queue.Queue()
            t1 = threading.Thread(target=action_google_true, args=(domain, userCountry, userServer,q1))
            t2 = threading.Thread(target=action_bing_true, args=(domain,q2))
            t1.start()
            t2.start()
            t1.join()
            t2.join()
            google_true_result = q1.get()
            bing_true_result = q2.get()
            time_spent_email = time.time() - start_time_email
            for emails, urls in google_results:
                email_address.append(emails)
            for emails in bing_true_result:
                email_address.append(emails)
            temp1 = [item.lower() for item in email_address]
            new_list = sorted(set(temp1))
            print '\nGathering Email Addresses From Google, Bing And LinkedIn'
            temp = 0
            for item in new_list:
                if item == '@' + domain or item == '@':
                    pass
                else:
                    temp += 1
                    print item
            print '\nPotential Emails Found: {}' .format(str(temp))
            time_spent_email_f = str(datetime.timedelta(seconds=(time_spent_email))).split('.')[0]
            print "\nEmail Enumeration: {0}\n".format(time_spent_email_f)         
            sys.exit()
        else:
            print colored('\n\tWild Cards Are Not In Place','red')
            email_address = []
            check_count = get_line_count(filename1)
            subs = get_subs(filename1)
            pool = ThreadPool(12)
            print '\nBrute Forcing Sub-Domains\n\n\tWhile you are waiting we will gather email addresses\n\tand LinkedIn users'
            print '\nGathering Email Addresses From Google, Bing And LinkedIn'
            userCountry, userServer = action_country_id()
            start_time_total = time.time()
            start_time_email = time.time()
            q1 = Queue.Queue()
            q2 = Queue.Queue()
            q3 = Queue.Queue()
            t1 = threading.Thread(target=action_google_true, args=(domain, userCountry, userServer,q1))
            t2 = threading.Thread(target=action_bing_true, args=(domain,q2))
            t3 = threading.Thread(target=action_linkedin, args=(domain,q3))
            t1.start()
            t2.start()
            t3.start()
            t1.join()
            t2.join()
            t3.join()
            time_spent_email = time.time() - start_time_email
            google_true_results = q1.get()
            bing_true_results = q2.get()
            linkedin_results = q3.get()
                      
            start_time_brute = time.time()
            pool.map(action_brute, subs)
            time_spent_brute = time.time() - start_time_brute
            pool.close()
            time_spent_total = time.time() - start_time_total
#Clean Brute Results           
            if targets == []:
                targets.append("temp-enter")
            domains = list(set(targets + netcraft_list))
            domains.sort()
            if "temp-enter" in domains: domains.remove("temp-enter")
            brute_results_dict = dict((x.split(' ') for x in domains))
#Outputing data
            action_output_wild_false(brute_results_dict, sub_intrest, google_true_results, bing_true_results, linkedin_results, check_count)
    else:
#Vuln Zone Trans
        print '\nGathering Email Addresses From Google and Bing'
        userCountry, userServer = action_country_id()
        start_time_email = time.time()
        t1 = threading.Thread(target=action_google_true, args=(domain, userCountry, userServer,))
        t1.start()
        t2 = threading.Thread(target=action_bing_true, args=(domain,))
        t2.start()
        t1.join()
        t2.join()
        result = q.get()
        time_spent_email = time.time() - start_time_email
        temp1 = [item.lower() for item in email_address]
        new_list = sorted(set(temp1))
        print '\nEmails Gathered From Google and Bing:\n'
        temp = 0
        for item in new_list:
            if item == '@' + domain or item == '@':
                pass
            else:
                temp += 1
                print item
        
        print '\nPotential Emails Found: {}' .format(str(temp))   
        time_spent_email_f = str(datetime.timedelta(seconds=(time_spent_email))).split('.')[0]
        print "\nEmail Enumeration: {0}\n".format(time_spent_email_f)